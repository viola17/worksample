{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import nltk\n",
    "from pomegranate import *\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pair.pkl','rb') as f:\n",
    "    pair = pickle.load(f)[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair.append([('UNK','N')])\n",
    "data = np.array(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kf = KFold(n_splits=5)\n",
    "accr = []\n",
    "for train, test in Kf.split(np.array(pair)):\n",
    "    trn = data[train]\n",
    "    tst = data[test]\n",
    "    ####TRAIN####\n",
    "    w_trn = [] #所有出现的词\n",
    "    p_trn = [] #所有词性\n",
    "    for i in trn:\n",
    "        for j in i:\n",
    "            w_trn.append(j[0])\n",
    "            p_trn.append(j[1])\n",
    "    \n",
    "    w_p_trn = defaultdict(list)\n",
    "    for i in range(len(w_trn)):\n",
    "        w_p_trn[w_trn[i]].append(p_trn[i])\n",
    "\n",
    "    #每个词性对应list of words\n",
    "    p_w_trn = defaultdict(list)\n",
    "    for i in range(len(w_trn)):\n",
    "        p_w_trn[p_trn[i]].append(w_trn[i])\n",
    "\n",
    "\n",
    "    w_dic2_trn = {}\n",
    "    for i in w_p_trn:\n",
    "        p_dic2_trn = {}\n",
    "        for j in w_p_trn[i]:\n",
    "            p_dic2_trn[j]=w_p_trn[i].count(j)\n",
    "        w_dic2_trn[i] = p_dic2_trn\n",
    "    #print(w_dic2)\n",
    "\n",
    "\n",
    "    p_dic1_trn = {}\n",
    "    for i in p_w_trn:\n",
    "        w_dic1_trn = {}\n",
    "        for j in p_w_trn[i]:\n",
    "            w_dic1_trn[j]=p_w_trn[i].count(j)\n",
    "        p_dic1_trn[i] = w_dic1_trn\n",
    "        \n",
    "    #emission matrix \n",
    "    p_freq_trn = {}\n",
    "    for i in p_dic1_trn:\n",
    "        p_count_trn = 0\n",
    "        word_trn = {}\n",
    "        for j in p_dic1_trn[i]:\n",
    "            p_count_trn += p_dic1_trn[i][j]\n",
    "        for j in p_dic1_trn[i]:\n",
    "            word_trn[j]=p_dic1_trn[i][j]/p_count_trn\n",
    "        p_freq_trn[i] = State(DiscreteDistribution(word_trn),name = i)\n",
    "\n",
    "    model_trn = HiddenMarkovModel('pos_tag_trn')\n",
    "\n",
    "    #共有词性多少个\n",
    "    count_trn = 0\n",
    "    for i in p_dic1_trn:\n",
    "        for j in p_dic1_trn[i]:\n",
    "            count_trn += p_dic1_trn[i][j]\n",
    "\n",
    "    states_trn = [] #pos tag\n",
    "    for i in p_freq_trn:\n",
    "        states_trn.append(p_freq_trn[i])\n",
    "\n",
    "    model_trn.add_states(states_trn)\n",
    "\n",
    "    for i in p_freq_trn:\n",
    "        model_trn.add_transition(model_trn.start, p_freq_trn[i], p_trn.count(i)/count_trn)\n",
    "\n",
    "    #transition matrix\n",
    "    p_sentence_trn =[]\n",
    "    w_sentence_trn = []\n",
    "    for i in trn:\n",
    "        p_string_trn = []\n",
    "        w_string_trn = []\n",
    "        for j in i:\n",
    "            p_string_trn.append(j[1])\n",
    "            w_string_trn.append(j[0])\n",
    "        p_sentence_trn.append(p_string_trn)\n",
    "        w_sentence_trn.append(w_string_trn)\n",
    "\n",
    "    trans_prob_trn = {}\n",
    "    for s in p_sentence_trn:\n",
    "        numerator_trn =0\n",
    "        for i in range(len(s)-1):\n",
    "            first_trn = s[i]\n",
    "            second_trn = s[i+1]\n",
    "            denom_trn = p_trn.count(first_trn)\n",
    "        if any([first_trn,second_trn] == s[i:i+2] for i in range(len(s) - 1)):\n",
    "            numerator_trn += 1\n",
    "        trans_prob_trn[(first_trn,second_trn)] = numerator_trn/denom_trn\n",
    "\n",
    "\n",
    "    for i in p_freq_trn:\n",
    "        prob_trn=0\n",
    "        for j in p_freq_trn:\n",
    "            if (i,j) not in trans_prob_trn.keys():\n",
    "                trans_prob_trn[(i,j)] = 0\n",
    "            model_trn.add_transition(p_freq_trn[i],p_freq_trn[j],trans_prob_trn[(i,j)])\n",
    "\n",
    "            prob_trn+=trans_prob_trn[(i,j)]\n",
    "        if prob_trn<1:\n",
    "            model_trn.add_transition(i,model_trn.end,1-prob_trn)\n",
    "    model_trn.bake()\n",
    "    \n",
    "    ####TEST####\n",
    "    #POS sentence list in test data\n",
    "    p_sentence_tst=[]\n",
    "    #word sentence list in test data\n",
    "    w_sentence_tst=[]\n",
    "    for i in tst:\n",
    "        p_string_tst = []\n",
    "        w_string_tst = []\n",
    "        for j in i:\n",
    "            p_string_tst.append(j[1])   \n",
    "            w_string_tst.append(j[0])\n",
    "        p_sentence_tst.append(p_string_tst)\n",
    "        w_sentence_tst.append(w_string_tst)\n",
    "    \n",
    "    correct = 0\n",
    "    tot = 0\n",
    "    for i in range(len(w_sentence_tst)):\n",
    "        tot+=len(w_sentence_tst[i]) #一句话的字符数量\n",
    "        for j in range(len(w_sentence_tst[i])):     \n",
    "            if w_sentence_tst[i][j] not in w_trn:\n",
    "                w_sentence_tst[i][j] = 'UNK'\n",
    "        pred = [state.name for q,state in model_trn.viterbi(w_sentence_tst[i])[1]]\n",
    "        pred.remove('pos_tag-start')\n",
    "        for pos in range(len(pred)):\n",
    "            if pred[pos] == p_sentence_tst[i][pos]:\n",
    "                correct += 1\n",
    "         \n",
    "    accr.append(correct/tot)\n",
    "\n",
    "print(mean(accr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1,2,3,4,5,6,7,8,9,9]\n",
    "X.remove(9)\n",
    "X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namae = [state.name for i,state in model.viterbi(['这个','词','的','意思','是'])[1]]\n",
    "namae.remove('pos_tag-start')\n",
    "print(namae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HMM(data):\n",
    "    w_data = [] #所有出现的词\n",
    "    p_data = [] #所有词性\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            w_data.append(j[0])\n",
    "            p_data.append(j[1])\n",
    "    \n",
    "    w_p_data = defaultdict(list)\n",
    "    for i in range(len(w_data)):\n",
    "        w_p_data[w_data[i]].append(p_data[i])\n",
    "\n",
    "    #每个词性对应list of words\n",
    "    p_w_data = defaultdict(list)\n",
    "    for i in range(len(w_data)):\n",
    "        p_w_data[p_data[i]].append(w_data[i])\n",
    "\n",
    "\n",
    "    w_dic2_data = {}\n",
    "    for i in w_p_data:\n",
    "        p_dic2_data = {}\n",
    "        for j in w_p_data[i]:\n",
    "            p_dic2_data[j]=w_p_data[i].count(j)\n",
    "        w_dic2_data[i] = p_dic2_data\n",
    "    #print(w_dic2)\n",
    "\n",
    "\n",
    "    p_dic1_data = {}\n",
    "    for i in p_w_data:\n",
    "        w_dic1_data = {}\n",
    "        for j in p_w_data[i]:\n",
    "            w_dic1_data[j]=p_w_data[i].count(j)\n",
    "        p_dic1_data[i] = w_dic1_data\n",
    "        \n",
    "    #emission matrix \n",
    "    p_freq_data = {}\n",
    "    for i in p_dic1_data:\n",
    "        p_count_data = 0\n",
    "        word_data = {}\n",
    "        for j in p_dic1_data[i]:\n",
    "            p_count_data += p_dic1_data[i][j]\n",
    "        for j in p_dic1_data[i]:\n",
    "            word_data[j]=p_dic1_data[i][j]/p_count_data\n",
    "        p_freq_data[i] = State(DiscreteDistribution(word_data),name = i)\n",
    "\n",
    "    model_data = HiddenMarkovModel('pos_tag_data')\n",
    "\n",
    "    #共有词性多少个\n",
    "    count_data = 0\n",
    "    for i in p_dic1_data:\n",
    "        for j in p_dic1_data[i]:\n",
    "            count += p_dic1_data[i][j]\n",
    "\n",
    "    states_data = [] #pos tag\n",
    "    for i in p_freq_data:\n",
    "        states.append(p_freq_data[i])\n",
    "\n",
    "    model_trn.add_states(states_data)\n",
    "\n",
    "    for i in p_freq_data:\n",
    "        model_data.add_transition(model_data.start, p_freq_data[i], p_data.count(i)/count_data)\n",
    "\n",
    "    #transition matrix\n",
    "    p_sentence_data =[]\n",
    "    for i in pair:\n",
    "        p_string_data = []\n",
    "        for j in i:\n",
    "            p_string_data.append(j[1])   \n",
    "        p_sentence_data.append(p_string_data)\n",
    "\n",
    "    trans_prob_data = {}\n",
    "    for s in p_sentence_data:\n",
    "        numerator_data =0\n",
    "        for i in range(len(s)-1):\n",
    "            first_data = s[i]\n",
    "            second_data = s[i+1]\n",
    "            denom_data = p_data.count(first)\n",
    "        if any([first_data,second_data] == s[i:i+2] for i in range(len(s) - 1)):\n",
    "            numerator_data += 1\n",
    "        trans_prob_data[(first_data,second_data)] = numerator_data/denom_data\n",
    "\n",
    "\n",
    "    for i in p_freq_data:\n",
    "        prob_data=0\n",
    "        for j in p_freq_data:\n",
    "            if (i,j) not in trans_prob_data.keys():\n",
    "                trans_prob_data[(i,j)] = 0\n",
    "            model_data.add_transition(p_freq_data[i],p_freq_data[j],trans_prob_data[(i,j)])\n",
    "\n",
    "            prob_data+=trans_prob_data[(i,j)]\n",
    "        if prob_data<1:\n",
    "            model_data.add_transition(i,model_data.end,1-prob_data)\n",
    "    return model_data.bake()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
